{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPqNAhcqjS2neRiDQU2uHDN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Course Name: **AI Mastery Bootcamp: AI Algorithms, DeepSeek AI, AI Agents**\n","\n","# Section 27: **Deep Dive into Queen 2.5**"],"metadata":{"id":"b5OE05IPYokJ"}},{"cell_type":"markdown","source":["## What is Qwen 2.5?\n","* Large Language Model (LLM) developed by Alibaba Cloud as part of the Tongyi Qianwen series\n","* Next-Generation A1 model optimized for text understanding, generation,reasoning, and code assistance\n","* Key Features of Qwen 2.5\n","  * Advanced Natural Language Processing (NLP)\n","  * Multilingual Support\n","  * Optimized for Code & Reasoning\n","  * Efficient and Faster\n","  * Multiple Model Sizes\n","  * Enhanced Security & Privacy\n","\n","### Qwen 2.5 vs. Other Models\n","\n","| **Feature** | **Qwen 2.5** | **Llama 3**| **GPT—4** | **Mistral** |\n","| ----------- | ------------ | ---------- | --------- | ----------- |\n","| **Developer** | Alibaba Cloud | Meta | OpenAl | Mistral AI |\n","| **Language Support** | Multilingual | English-focused | Multilingual | Multilingual|\n","| **Fine-Tuning** | Yes | Yes | Limited | Yes |\n","| **Efficiency** | High | High | Moderate | High |\n","| **Best Use Cases** | Chatbots, Coding | Chatbots, NLP | Advanced AI | Lightweight |\n","\n","### What is Ollama?\n","* Open-source runtime that enables easy deployment and execution of large language models (LLMs) locally on your machine\n","* Simple interface to run, manage, and interact with models like Qwen, Llama, Mistral, Gemma, and more\n","* Key Features of Ollama\n","  * Local LLM Execution\n","  * Model Management\n","  * Simple CLI Interface\n","  * Fast & Optimized\n","  * Developer-Friendly\n","  * Supports Multiple Models\n","\n","### Why Use Ollama?\n","| **Feature** | **Ollama** | **LangChain** | **OpenAi API** |\n","| ----------- | ---------- | ------------- | -------------- |\n","| Runs Locally? | Yes |  No | No |\n","| Free to Use? | Yes |  Yes | No (Paid) |\n","| Supports Custom Models? | Yes |  Yes | No (Closed) |\n","| Optimized for Speed? | Yes | Varies |  Yes |\n","\n","\n","### How Do Qwen 2.5 & Ollama Work Together?\n","* Step-by-Step Process\n","  1. Ollama manages and runs Qwen 2.5 locally\n","  2. User sends a prompt to Qwen 2.5 via Ollama's API\n","  3. Qwen 2.5 processes the input and generates a response\n","  4. The output is returned to the user through FastAPl or a chatbot Ul\n","\n","Think of Ollama as the \"engine\" and Qwen 2.5 as the \"brain\" that powers your AI application!\n","\n","### Summary\n","* Qwen 2.5 + Ollama is a powerful combination for building AI applications locally without relying on cloud services like OpenAl\n","* Qwen 2.5 —A high-performance LLM for chatbots, coding, and automation\n","* Ollama — A local A1 runtime for running models securely and efficiently\n","* Want to build something cool? Let's start a hands-on project!\n","\n","### What You Will Learn\n","* Setting up Ollama and Qwen 2.5 on a local machine\n","* Using FastAPl to create a chatbot backend\n","* Creating a React.js frontend for user interaction\n","* Deploying the chatbot locally"],"metadata":{"id":"LkyG83KMY9WT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NH9SN77EYlHG"},"outputs":[],"source":["from fastapi import FastAPI, HTTPException\n","from fastapi.middleware. cors import CORSMiddleware\n","from pydantic import BaseModel\n","import ollama\n","app= FastAPI()\n","\n","# Enable CORS to allow frontend to access the backend\n","\n","app.add_middleware(\n","    CORSMiddleware,\n","    allow_origins=[\"*\"],     # Allow all origins (or specify ['http://localhost:300'] for security)\n","    allow_credentials=True,\n","    allow_methods=[\"*\"],     # Allow all HTTP method\n","    allow_headers=[\"*\"],     # ALlow all headers\n",")\n","\n","class ChatRequest(BaseModel):\n","    prompt: str\n","\n","@app.post(\"/chat\")\n","async def chat(request: ChatRequest):\n","  try:\n","    reponse= ollama.chat(model=\"qwen2.5\", messages= [{\"role\": \"user\", \"content\": request.prompt}])\n","    return {\"response\": response}\n","  except Exception as e:\n","    raise HTTPException(status_code=500, detail=str(e))\n","\n","@app.get(\"/\")\n","def home():\n","  return {\"message\": \"Qwen 2.5 Chatbot API is Running!\"}"]}]}