{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMyiIeRU5TTtfbWkWlrE4wZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Course Name: **AI Mastery Bootcamp: AI Algorithms, DeepSeek AI, AI Agents**\n","\n","# Section 19: **Practical Applications and Projects**"],"metadata":{"id":"ytwZzUln_akY"}},{"cell_type":"markdown","source":["## Real-world Applications: Image Classification\n","1.. Healthcare\n","  * Medical Imaging Diagnosis\n","  * Pathology Analysis\n","2. Automotive\n","  * Autonomous Driving\n","  * Driver Monitoring Systems\n","3. Retail\n","  * Product Recognition\n","  * Customer Behavior Analysis\n","4. Agriculture\n","  * Crop Monitoring\n","  * Weed Detection\n","5. Security and Surveillance\n","  * Facial Recognition\n","  * Object Detection\n","\n","## Real-world Applications: Natural Language Processing\n","1. Healthcare\n","  * Clinical Document Analysis\n","  * Drug Discovery\n","2. Finance\n","  * Sentiment Analysis\n","  * Fraud Detection\n","3. Customer Service\n","  * Chatbots and Virtual Assistants\n","  * Sentiment Analysis\n","4. E-commerce\n","  * Product Recommendations\n","  * Product Description Analysis\n","5. Legal and Compliance\n","  * Contract Analysis\n","  * Compliance Monitoring\n","\n","## Real-world Applications: Recommender Systems\n","1. E-commerce\n","  * Product Recommendations\n","  * Personalized Shopping Experience\n","2. Streaming Media\n","  * Content Recommendations\n","  * Personalized Playlists\n","3. Social Media\n","  * Friend Recommendations\n","  * Content Sharing\n","4. Travel and Hospitality\n","  * Hotel and Travel Recommendations\n","  * Activity and Experience Recommendations\n","5. Online Education\n","  * Course Recommendations\n","  * Learning Content Personalization\n","\n","## Real-world Applications: Object Detection\n","1. Autonomous Vehicles\n","  * Pedestrian Detection\n","  * Traffic Sign Recognition\n","2. Surveillance and Security\n","  * Intrusion Detection\n","  * Object Tracking\n","3. Retail and Inventory Management\n","  * Customer Tracking\n","  * Shelf Monitoring\n","4. Industrial Automation\n","  * Defect Detection\n","  * Object Sorting and Handling\n","5. Healthcare\n","  * Medical Imaging Analysis\n","  * Surgical Assistance"],"metadata":{"id":"lQHcBhSL_pdF"}},{"cell_type":"markdown","source":["## Building a Sentiment Analysis Model\n","* Step 1: Data Preparation\n","  * Data Collection\n","  * Data Preprocessing\n","* Step 2: Building the Neural Network Model\n","  * Define Model Architecture\n","  * Compile the Model\n","* Step 3: Training the Model\n","  * Split the Data\n","  * Train the Model\n","* Step 4: Model Evaluation\n","  * Evaluate Performance\n","* Step 5: Model Deployment\n","  * Save the Model\n","  * Deploy the Model\n","* Step 6: Continuous Improvement\n","  * Fine-Tuning\n","  * Update Model"],"metadata":{"id":"TPyYEKxtAusr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ke2UWyKh_Xn8"},"outputs":[],"source":["import  tensorflow as tf\n","from tensflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Step 1: Data Preparation\n","## Assuming 'texts' contains list of text documents and 'labels' contains corresponding sentiment labels\n","\n","### Tokenize and pas sequence\n","tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(texts)\n","sequences = tokenizer.texts_to_sequences(texts)\n","padded_sequences = pad_sequences(sequences, maxlen=100, padding='post', truncating='post')\n","\n","# Step 2: Building the Neural Network Model\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(input_dim=10000, output_dim=16, input_length=100),\n","    tf.keras.layers.Birectional(tf.keras.layers.LSTM),\n","    tf.keras.layers.Dense(64, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Step 3: Training the Model\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n","\n","history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)\n","\n","# Step 4: Model Evaluation\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n","\n","# Step 5: Model Deployment\n","model.save('sentiment_analysis_model.h5')\n","\n","# Step 6: Continuous Improvement\n","## Fine-tune the model, update data, and retrain periodically\n"]},{"cell_type":"markdown","source":["## Creating an Image Recognition System\n","* Step 1: Data Collection and Preparation\n","  * Dataset Selection\n","  * Data Preprocessing\n","* Step 2: Building the Convolutional Neural Network (CNN) Model\n","  * Model Architecture\n","  * Model Definition\n","* Step 3: Model Training\n","  * Data Augmentation\n","  * Compile the Model\n","  * Training\n","* Step 4: Model Evaluation\n","  * Validation\n","  * Fine-Tuning\n","* Step 5: Model Deployment\n","  * Save the Model\n","  * Deployment\n","* Step 6: Continuous Improvement\n","  *Monitoring and Maintenance"],"metadata":{"id":"0IjShtLBDJlR"}},{"cell_type":"code","source":["import  tensorflow as tf\n","from tensorflow.keras import layers, models\n","\n","# Step 2: Building the CNN Model\n","model= model.Sequential([\n","    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)).\n","    layers.MaxPooling2D((2, 2)),\n","    layers..Conv2D(32, (3, 3), activation='relu'),\n","    layers.MaxPooling2D((2, 2)),\n","    layers..Conv2D(32, (3, 3), activation='relu'),\n","    layers.Flatten(),\n","    layers.Dense(64, activation= 'relu'),\n","    layers.Dense(10, activation= 'softmax')\n","])\n","\n","# Step 3: Training the Model\n","model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","history= model.fit(train_images, train_labels, epochs=10, validation_data= (test_images, test_labels))\n","\n","# Step 4: Model Evaluation\n","loss, accuracy = model.evaluate(test_images, test_labels)\n","print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n","\n","# Step 5: Model Deployment\n","model.save('image_recognition_model.h5')\n","\n","# Step 6: Continuous Improvement\n","## Fine-tune the model, update data, and retrain periodically\n"],"metadata":{"id":"5bnm6RfC_hYp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Developing a Time Series Prediction Model\n","* Step 1: Data Collection and Preparation\n","  * Dataset Selection\n","  * Data Preprocessing\n","* Step 2: Building the Recurrent Neural Network (RNN) Model\n","  * Model Architecture\n","  * Model Definition\n","* Step 3: Model Training\n","  * Data Preparation\n","  * Compile the Model\n","  * Training\n","* Step 4: Model Evaluation\n","  * Validation\n","  * Fine-Tuning\n","* Step 5: Model Deployment\n","  * Save the Model\n","  * Deployment\n","* Step 6: Continuous Improvement\n","  * Monitoring and Maintenance"],"metadata":{"id":"Imfpqqy3J2ct"}},{"cell_type":"code","source":["import  tensorflow as tf\n","from tensorflow.keras import layers, models\n","\n","# Step 2: Building the RNN Model\n","model= models.Sequential([\n","    layers.LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2])),\n","    layers.Dense(1)\n","])\n","\n","# Step 3: Training the Model\n","model.compile(loss='mse', optimizer='adam')\n","\n","history= model.fit(X_train, y_train, epochs=10, validation_data= (X_valid, y_valid))\n","\n","# Step 4: Model Evaluation\n","loss = model.evaluate(X_test, y_test)\n","print(f\"Test Loss: {loss}\")\n","\n","# Step 5: Model Deployment\n","model.save('time_series_predictoion_model.h5')\n","\n","# Step 6: Continuous Improvement\n","## Fine-tune the model, update data, and retrain periodically"],"metadata":{"id":"zU83wXoyKENB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Implementing a Chatbot\n","* Step 1: Data Collection and Preprocessing\n","  * Dataset Selection\n","  * Data Preprocessing\n","* Step 2: Building the Sequence-to-Sequence Model\n","  * Model Architecture\n","  * Model Definition\n","* Step 3: Model Training\n","  * Data Preparation\n","  * Compile the Model\n","  * Training\n","* Step 4: Model Evaluation\n","  * Validation\n","* Step 5: Chatbot Interaction\n","  * User Input\n","  * Model Inference\n","  * Output"],"metadata":{"id":"D8kB3UzBLCN8"}},{"cell_type":"code","source":["import  tensorflow as tf\n","from tensorflow.keras.layers import Input, LSTM, Dense\n","from tensorflow.keras.models import Model\n","\n","# Define the Seq2Seq Architecture\n","latent_dim= 256\n","encoder_inputs= Input(shape= (None, num_encoder_tokens))\n","encoder= LSTM(latent_dim, return_state= True)\n","encoder_outputs, state_h, state_c= encoder(encoder_inputs)\n","encoder_states= [state_h, state_c]\n","\n","decoder_inputs= Input(shape= (None, num_decoder_tokens))\n","decoder_lstm= LSTM(latent_dim, return_sequences= True, return_state= True)\n","decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state= encoder_states)\n","decoder_dense= Dense(num_decoder_tokens, activation= 'softmax')\n","decoder_outputs= decoder_dense(decoder_outputs)\n","\n","model= Model([encode_inputs, decoder_inputs], decoder_outputs)\n","\n","# Compile the model\n","model.compile(optimizer- 'rmsprop', loss='categorical_crossentropy')\n","\n","# Train the model\n","model.fit([encode_inputs_data, decoder_inputs_data], decoder_target_data,\n","          batch_size= batch_size,\n","          epochs= epochs,\n","          validation_split= 0.2)\n","# Save the model\n","model.save('chatbot_model.h5')\n","\n","# Implement chatbot iteraction using the trained model\n","def chatbot_response(input_text):\n","  # Preprocess input_text (tokenizer, padding, etc.)\n","  # Encode input_text using the encoder model\n","  # Generate response using the decoder model\n","  # Decode response token into text\n","  return generated_response"],"metadata":{"id":"yve2Wjk8LMcH"},"execution_count":null,"outputs":[]}]}